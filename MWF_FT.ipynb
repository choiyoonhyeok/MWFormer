{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "authorship_tag": "ABX9TyOk64cscuih0ORN3QQ6uA0T",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/choiyoonhyeok/MWFormer/blob/main/MWF_FT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Srs_Ru4dxXTF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52406c2e-c26f-44a1-e531-f43de0881775"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install wandb"
      ],
      "metadata": {
        "id": "gBC3lp45WFOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "# ë¡œê·¸ì¸(ìµœì´ˆ 1íšŒë§Œ ë¸Œë¼ìš°ì € ì¸ì¦). ì´ë¯¸ ë¡œê·¸ì¸ë¼ ìˆìœ¼ë©´ ê±´ë„ˆëœ€.\n",
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "id": "TAIZ88DOWIgZ",
        "outputId": "cdc5a68e-684a-45cf-b3b4-ddc4ba4c03a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mchoiyoonhyeok0427\u001b[0m (\u001b[33mchoiyh0427-medical-ai\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPU/íŒŒì´ì¬ í™˜ê²½ í™•ì¸"
      ],
      "metadata": {
        "id": "jgJD8nKMbpyL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "YHNGuteBJtOJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12e11acd-07f0-4639-d1af-c0c217f5d39d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Aug 25 18:01:14 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA L4                      Off |   00000000:00:03.0 Off |                    0 |\n",
            "| N/A   45C    P8             12W /   72W |       0MiB /  23034MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, platform, sys\n",
        "print(\"Python:\", sys.version)\n",
        "print(\"PyTorch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L0ZEcrI8bmo2",
        "outputId": "6bbd1279-4cfc-418c-ab45-0f47823652fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python: 3.12.11 (main, Jun  4 2025, 08:56:18) [GCC 11.4.0]\n",
            "PyTorch: 2.8.0+cu126\n",
            "CUDA available: True\n",
            "GPU: NVIDIA L4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ë ˆí¬ í´ë¡ "
      ],
      "metadata": {
        "id": "cB9TtweNbr-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ê¹”ë”í•œ ì‘ì—… ë””ë ‰í† ë¦¬\n",
        "%cd /content\n",
        "\n",
        "# ë³¸ì¸ fork ë ˆí¬ í´ë¡ \n",
        "!git clone https://github.com/choiyoonhyeok/MWFormer.git\n",
        "%cd MWFormer\n",
        "!git rev-parse --short HEAD"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "33A8zu_KbsbP",
        "outputId": "af7933d3-0b4b-40c7-caba-93eda5918496"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n",
            "Cloning into 'MWFormer'...\n",
            "remote: Enumerating objects: 286, done.\u001b[K\n",
            "remote: Counting objects: 100% (286/286), done.\u001b[K\n",
            "remote: Compressing objects: 100% (266/266), done.\u001b[K\n",
            "remote: Total 286 (delta 29), reused 265 (delta 16), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (286/286), 24.33 MiB | 17.21 MiB/s, done.\n",
            "Resolving deltas: 100% (29/29), done.\n",
            "/content/MWFormer\n",
            "5128e95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜"
      ],
      "metadata": {
        "id": "nXqEsyecbzbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# requirements.txt ìˆìœ¼ë©´ ì„¤ì¹˜\n",
        "!if [ -f requirements.txt ]; then \\\n",
        "    echo \"requirements.txt detected â†’ installing...\"; \\\n",
        "    pip install -q -r requirements.txt; \\\n",
        "  else \\\n",
        "    echo \"requirements.txt not found â†’ skip\"; \\\n",
        "  fi\n",
        "\n",
        "# ê³µí†µ ì˜ì¡´ì„± (ëˆ„ë½ ëŒ€ë¹„)\n",
        "!pip install -q opencv-python einops timm yacs tqdm thop tensorboard albumentations"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kk7Uh92Pbz8N",
        "outputId": "40b89719-de25-4849-e367-2cd9b55769e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "requirements.txt not found â†’ skip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Google Drive ê²½ë¡œ ë³€ìˆ˜ ë“±ë¡ & ì¡´ì¬ ì—¬ë¶€ ì ê²€"
      ],
      "metadata": {
        "id": "zcelYGIMb7uw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# === ê²½ë¡œ ì„¤ì • ===\n",
        "DRIVE_ROOT = Path(\"/content/drive/MyDrive/BOAZminiproject1\")\n",
        "\n",
        "DATA_ROOT = DRIVE_ROOT / \"fulldata\"  # rain/fog/dust í¬í•¨\n",
        "WEIGHT_BACKBONE = DRIVE_ROOT / \"MWFormer_weights\" / \"MWFormer_L\" / \"backbone.pth\"\n",
        "WEIGHT_STYLE    = DRIVE_ROOT / \"MWFormer_weights\" / \"pretrained_feature_extraction_network\" / \"style_filter.pth\"\n",
        "\n",
        "print(\"DATA_ROOT        :\", DATA_ROOT)\n",
        "print(\"WEIGHT_BACKBONE  :\", WEIGHT_BACKBONE)\n",
        "print(\"WEIGHT_STYLE     :\", WEIGHT_STYLE)\n",
        "\n",
        "# === ì¡´ì¬ ì—¬ë¶€ ì ê²€ ===\n",
        "assert DATA_ROOT.exists(), f\"ë°ì´í„° í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤: {DATA_ROOT}\"\n",
        "assert WEIGHT_BACKBONE.exists(), f\"ë°±ë³¸ ê°€ì¤‘ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤: {WEIGHT_BACKBONE}\"\n",
        "assert WEIGHT_STYLE.exists(),    f\"style_filter ê°€ì¤‘ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤: {WEIGHT_STYLE}\"\n",
        "print(\"âœ… ëª¨ë“  ê²½ë¡œê°€ ì •ìƒì…ë‹ˆë‹¤.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b-ZLYb9Kb51F",
        "outputId": "d6ffc575-b1d2-4414-ea55-d14742244951"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DATA_ROOT        : /content/drive/MyDrive/BOAZminiproject1/fulldata\n",
            "WEIGHT_BACKBONE  : /content/drive/MyDrive/BOAZminiproject1/MWFormer_weights/MWFormer_L/backbone.pth\n",
            "WEIGHT_STYLE     : /content/drive/MyDrive/BOAZminiproject1/MWFormer_weights/pretrained_feature_extraction_network/style_filter.pth\n",
            "âœ… ëª¨ë“  ê²½ë¡œê°€ ì •ìƒì…ë‹ˆë‹¤.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ë§¤ì¹­ ë¦¬ìŠ¤íŠ¸ ìƒì„± ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰"
      ],
      "metadata": {
        "id": "0i6PY-LtcUmr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3ë‹¨ê³„: ë“œë¼ì´ëŸ°(sanity check) í•™ìŠµ"
      ],
      "metadata": {
        "id": "PLzSCRgbqqdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === MWFormer íŒ¨ì¹˜ ì˜¬ì¸ì› ì…€ ===\n",
        "# - main_finetune.py\n",
        "#   (1) ì²´í¬í¬ì¸íŠ¸ ìœ ì—° ë¡œë”(_load_state_dict_flexible) ì‚½ì…\n",
        "#   (2) .module.state_dict() ì‚¬ìš©ë¶€ ì¹˜í™˜\n",
        "#   (3) TrainingDataset íƒ€ì… 0/1/2 â†’ 3ìœ¼ë¡œ í†µì¼\n",
        "#   (4) W&B ë¡œê¹…: wandb.init / wandb.log(loss/lr/iter, initial PSNR) / wandb.finish()\n",
        "#      * AMP(GradScaler) ì‚¬ìš© ì¤‘ì´ë©´ scaler.update() ë’¤ì— ë¡œê·¸, ì•„ë‹ˆë©´ opts.step() ë’¤ì— ë¡œê·¸\n",
        "# - train_data_functions.py\n",
        "#   (5) 'is not 3' â†’ '!= 3'\n",
        "#   (6) ë¦¬ìŠ¤íŠ¸ íŒŒì„œ: 'ë‘ ì¹¼ëŸ¼ input  GT' ë˜ëŠ” 'í•œ ì¹¼ëŸ¼ input' ëª¨ë‘ ì§€ì›\n",
        "#       + train_data_dir ëì— '/' ê°•ì œ + length ì¶œë ¥\n",
        "# - utils_network.py\n",
        "#   (7) SSIM/PSNRì— data_range=1.0 ì§€ì •(ì¤‘ë³µ ë°©ì§€ í¬í•¨)\n",
        "\n",
        "from pathlib import Path\n",
        "import re\n",
        "\n",
        "# ---------- 1) main_finetune.py íŒ¨ì¹˜ ----------\n",
        "mf = Path(\"/content/MWFormer/main_finetune.py\")\n",
        "msrc = mf.read_text()\n",
        "\n",
        "# (1) ìœ ì—° ë¡œë” í—¬í¼ ì‚½ì… (import torch ë°”ë¡œ ë’¤)\n",
        "if \"_load_state_dict_flexible\" not in msrc:\n",
        "    helper = r'''\n",
        "from collections import OrderedDict\n",
        "\n",
        "def _load_state_dict_flexible(ckpt_path, map_location=\"cpu\"):\n",
        "    obj = torch.load(ckpt_path, map_location=map_location)\n",
        "    if hasattr(obj, \"state_dict\") and callable(getattr(obj, \"state_dict\")):\n",
        "        sd = obj.state_dict()\n",
        "    elif isinstance(obj, dict) and \"state_dict\" in obj and isinstance(obj[\"state_dict\"], (dict, OrderedDict)):\n",
        "        sd = obj[\"state_dict\"]\n",
        "    elif isinstance(obj, dict) and \"model\" in obj and isinstance(obj[\"model\"], (dict, OrderedDict)):\n",
        "        sd = obj[\"model\"]\n",
        "    elif isinstance(obj, (dict, OrderedDict)):\n",
        "        sd = obj\n",
        "    else:\n",
        "        try:\n",
        "            sd = obj.module.state_dict()\n",
        "        except Exception:\n",
        "            sd = obj\n",
        "    new_sd = OrderedDict()\n",
        "    for k, v in sd.items():\n",
        "        nk = k[7:] if k.startswith(\"module.\") else k\n",
        "        new_sd[nk] = v\n",
        "    return new_sd\n",
        "'''\n",
        "    msrc = re.sub(r\"(import\\s+torch[^\\n]*\\n)\", r\"\\1\" + helper + \"\\n\", msrc, count=1)\n",
        "\n",
        "# (2) .module.state_dict() â†’ _load_state_dict_flexible(...)\n",
        "msrc = re.sub(\n",
        "    r\"torch\\.load\\(([^)]*)\\)\\.module\\.state_dict\\(\\)\",\n",
        "    r\"_load_state_dict_flexible(\\1)\",\n",
        "    msrc\n",
        ")\n",
        "\n",
        "# (3) Dataset íƒ€ì… í†µì¼: 0/1/2 â†’ 3\n",
        "msrc = msrc.replace(\"TrainingDataset(0, \", \"TrainingDataset(3, \")\n",
        "msrc = msrc.replace(\"TrainingDataset(1, \", \"TrainingDataset(3, \")\n",
        "msrc = msrc.replace(\"TrainingDataset(2, \", \"TrainingDataset(3, \")\n",
        "\n",
        "# (4-1) W&B import\n",
        "if \"import wandb\" not in msrc:\n",
        "    msrc = msrc.replace(\"from datetime import datetime\", \"from datetime import datetime\\nimport wandb\")\n",
        "\n",
        "# (4-2) run_name ì§í›„ wandb.init + LOG_EVERY\n",
        "if \"wandb.init(\" not in msrc:\n",
        "    msrc = msrc.replace(\n",
        "        \"run_name = f'{args.file_name}-{now}'\",\n",
        "        \"run_name = f'{args.file_name}-{now}'\\n\"\n",
        "        \"    # === W&B init ===\\n\"\n",
        "        \"    wandb.init(project='MWFormer', name=run_name,\\n\"\n",
        "        \"               config={\\n\"\n",
        "        \"                   'file_name': args.file_name,\\n\"\n",
        "        \"                   'num_steps': args.num_steps,\\n\"\n",
        "        \"                   'learning_rate': args.learning_rate,\\n\"\n",
        "        \"                   'crop_size': args.crop_size,\\n\"\n",
        "        \"                   'num_workers': args.num_workers,\\n\"\n",
        "        \"                   'snapshot_dir': args.snapshot_dir,\\n\"\n",
        "        \"               })\\n\"\n",
        "        \"    LOG_EVERY = 100  # wandb ë¡œê·¸ ì£¼ê¸°(steps)\\n\"\n",
        "    )\n",
        "\n",
        "# (4-3) ì´ˆê¸° PSNR wandb.log\n",
        "if \"val_psnr_initial\" not in msrc:\n",
        "    msrc = msrc.replace(\n",
        "        \"print('initial model PSNR: ', old_val_psnr)\",\n",
        "        \"print('initial model PSNR: ', old_val_psnr)\\n\"\n",
        "        \"    try:\\n\"\n",
        "        \"        wandb.log({'val_psnr_initial': float(old_val_psnr)}, step=0)\\n\"\n",
        "        \"    except Exception:\\n\"\n",
        "        \"        pass\"\n",
        "    )\n",
        "\n",
        "# (4-4) í•™ìŠµ ë£¨í”„ ë‚´ wandb.log(loss/lr/iter)\n",
        "# AMPê°€ ì ìš©ë˜ì–´ ìˆìœ¼ë©´ scaler.update() ë’¤ì—, ì—†ìœ¼ë©´ opts.step() ë’¤ì— ì‚½ì…\n",
        "if \"scaler.update()\" in msrc and \"wandb.log({\" not in msrc:\n",
        "    msrc = msrc.replace(\n",
        "        \"scaler.update()\",\n",
        "        \"scaler.update()\\n\"\n",
        "        \"        # === W&B log ===\\n\"\n",
        "        \"        try:\\n\"\n",
        "        \"            if (i_iter % LOG_EVERY) == 0:\\n\"\n",
        "        \"                wandb.log({'loss': float(loss_p.detach().item()), 'lr': float(main_lr), 'iter': int(i_iter)})\\n\"\n",
        "        \"        except Exception:\\n\"\n",
        "        \"            pass\"\n",
        "    )\n",
        "elif \"wandb.log({\" not in msrc and \"opts.step()\" in msrc:\n",
        "    msrc = msrc.replace(\n",
        "        \"opts.step()\",\n",
        "        \"opts.step()\\n\"\n",
        "        \"        # === W&B log ===\\n\"\n",
        "        \"        try:\\n\"\n",
        "        \"            if (i_iter % LOG_EVERY) == 0:\\n\"\n",
        "        \"                wandb.log({'loss': float(loss_p.detach().item()), 'lr': float(main_lr), 'iter': int(i_iter)})\\n\"\n",
        "        \"        except Exception:\\n\"\n",
        "        \"            pass\"\n",
        "    )\n",
        "\n",
        "# (4-5) ì¢…ë£Œ ì‹œ wandb.finish()\n",
        "if \"wandb.finish()\" not in msrc:\n",
        "    msrc += \"\\ntry:\\n    pass\\nfinally:\\n    import wandb as _w; _w.finish()\\n\"\n",
        "\n",
        "mf.write_text(msrc)\n",
        "print(\"âœ… main_finetune.py íŒ¨ì¹˜ ì™„ë£Œ (ìœ ì—° ë¡œë” + íƒ€ì… í†µì¼ + W&B ë¡œê¹…)\")\n",
        "\n",
        "# ---------- 2) train_data_functions.py íŒ¨ì¹˜ ----------\n",
        "tp = Path(\"/content/MWFormer/train_data_functions.py\")\n",
        "tsrc = tp.read_text()\n",
        "\n",
        "# (5) 'is not 3' ê²½ê³  ìˆ˜ì •\n",
        "tsrc = tsrc.replace(\" is not 3\", \" != 3\")\n",
        "\n",
        "# (6) ë¦¬ìŠ¤íŠ¸ íŒŒì„œ êµì²´ (with open(train_list) ~ self.train_data_dir ì„¤ì •ê¹Œì§€)\n",
        "lines = tsrc.splitlines()\n",
        "start_idx = None\n",
        "end_idx = None\n",
        "for i, ln in enumerate(lines):\n",
        "    if \"with open(train_list) as f:\" in ln and start_idx is None:\n",
        "        start_idx = i\n",
        "    if start_idx is not None and \"self.train_data_dir = train_data_dir\" in ln:\n",
        "        end_idx = i\n",
        "        break\n",
        "\n",
        "if start_idx is not None and end_idx is not None:\n",
        "    new_block = [\n",
        "        \"        with open(train_list) as f:\",\n",
        "        \"            contents = [ln.strip() for ln in f if ln.strip()]\",\n",
        "        \"\",\n",
        "        \"        input_names = []\",\n",
        "        \"        gt_names    = []\",\n",
        "        \"\",\n",
        "        \"        for ln in contents:\",\n",
        "        \"            parts = ln.split()  # ê³µë°±/íƒ­ ë¶„ë¦¬\",\n",
        "        \"            if len(parts) >= 2:\",\n",
        "        \"                # ë‘ ì¹¼ëŸ¼: 0=ì…ë ¥, 1=GT\",\n",
        "        \"                inp_rel = parts[0].lstrip('/')\",\n",
        "        \"                gt_rel  = parts[1].lstrip('/')\",\n",
        "        \"                input_names.append(inp_rel)\",\n",
        "        \"                gt_names.append(gt_rel)\",\n",
        "        \"            else:\",\n",
        "        \"                # í•œ ì¹¼ëŸ¼: ì…ë ¥ë§Œ â†’ GTëŠ” inputâ†’gt ì¹˜í™˜\",\n",
        "        \"                inp_rel = parts[0].lstrip('/')\",\n",
        "        \"                gt_rel  = inp_rel.replace('/input/', '/gt/').replace('/GT/', '/gt/')\",\n",
        "        \"                input_names.append(inp_rel)\",\n",
        "        \"                gt_names.append(gt_rel)\",\n",
        "        \"\",\n",
        "        \"        self.input_names = input_names\",\n",
        "        \"        self.gt_names    = gt_names\",\n",
        "        \"        self.crop_size   = crop_size\",\n",
        "        \"        self.train_data_dir = train_data_dir if train_data_dir.endswith('/') else (train_data_dir + '/')\",\n",
        "        \"\",\n",
        "        \"        print('length of input names:', len(input_names))\",\n",
        "    ]\n",
        "    tsrc = \"\\n\".join(lines[:start_idx] + new_block + lines[end_idx+1:])\n",
        "else:\n",
        "    print(\"âš ï¸ ë¦¬ìŠ¤íŠ¸ íŒŒì„œ ë¸”ë¡ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. íŒŒì¼ êµ¬ì¡°ê°€ ë‹¬ë¼ì¡Œì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (íŒ¨ì¹˜ ê±´ë„ˆëœ€)\")\n",
        "\n",
        "tp.write_text(tsrc)\n",
        "print(\"âœ… train_data_functions.py íŒ¨ì¹˜ ì™„ë£Œ (ë¦¬ìŠ¤íŠ¸ íŒŒì„œ ìœ ì—°í™”)\")\n",
        "\n",
        "# ---------- 3) utils_network.py íŒ¨ì¹˜ ----------\n",
        "up = Path(\"/content/MWFormer/utils_network.py\")\n",
        "usrc = up.read_text()\n",
        "\n",
        "# compare_ssim/compare_psnr â†’ data_range=1.0 ì§€ì •\n",
        "usrc = re.sub(r\"compare_ssim\\(([^)]*)\\)\", r\"compare_ssim(\\1, data_range=1.0)\", usrc)\n",
        "usrc = re.sub(r\"compare_psnr\\(([^)]*)\\)\", r\"compare_psnr(\\1, data_range=1.0)\", usrc)\n",
        "\n",
        "# í˜¹ì‹œ ì´ì „ ì¤‘ë³µ ì¸ìê°€ ë‚¨ì•„ìˆë‹¤ë©´ ì •ë¦¬\n",
        "usrc = usrc.replace(\"data_range=1, data_range=1.0\", \"data_range=1.0\")\n",
        "usrc = usrc.replace(\"data_range=1.0, data_range=1.0\", \"data_range=1.0\")\n",
        "\n",
        "up.write_text(usrc)\n",
        "print(\"âœ… utils_network.py íŒ¨ì¹˜ ì™„ë£Œ (SSIM/PSNR data_range=1.0)\")\n",
        "\n",
        "print(\"\\nğŸ‰ ëª¨ë“  íŒ¨ì¹˜ ì™„ë£Œ! ì´ì œ W&B ë¡œê¹…ê³¼ ìœ ì—°í•œ ë°ì´í„° ë¡œë”©ì´ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGsvFpX_JjTW",
        "outputId": "ec74e437-8085-47c9-b7b5-49a7e1be0fe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… main_finetune.py íŒ¨ì¹˜ ì™„ë£Œ (ìœ ì—° ë¡œë” + íƒ€ì… í†µì¼ + W&B ë¡œê¹…)\n",
            "âœ… train_data_functions.py íŒ¨ì¹˜ ì™„ë£Œ (ë¦¬ìŠ¤íŠ¸ íŒŒì„œ ìœ ì—°í™”)\n",
            "âœ… utils_network.py íŒ¨ì¹˜ ì™„ë£Œ (SSIM/PSNR data_range=1.0)\n",
            "\n",
            "ğŸ‰ ëª¨ë“  íŒ¨ì¹˜ ì™„ë£Œ! ì´ì œ W&B ë¡œê¹…ê³¼ ìœ ì—°í•œ ë°ì´í„° ë¡œë”©ì´ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "BASE = Path(\"/content/drive/MyDrive/BOAZminiproject1\")\n",
        "SRC  = BASE / \"finetunning\" / \"train_pairs_joint.txt\"  # ì „ì²´ ë¦¬ìŠ¤íŠ¸ (ë‘ ì¹¼ëŸ¼)\n",
        "DST_DIR = BASE / \"fulldata\" / \"lists\"\n",
        "DST_DIR.mkdir(parents=True, exist_ok=True)\n",
        "DST  = DST_DIR / \"train_pairs_joint.small.txt\"\n",
        "\n",
        "# ì• 800ì¤„ë§Œ ë³µì‚¬ (ì´ë¯¸ ìˆìœ¼ë©´ ê±´ë„ˆëœ€)\n",
        "if SRC.exists() and not DST.exists():\n",
        "    n = 800\n",
        "    with open(SRC) as f, open(DST, \"w\") as w:\n",
        "        for i, line in enumerate(f):\n",
        "            if i >= n: break\n",
        "            w.write(line)\n",
        "print(\"ìƒ˜í”Œ ë¦¬ìŠ¤íŠ¸ ìœ„ì¹˜:\", DST)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rstz4D76Ly0J",
        "outputId": "36df86a0-9660-4a13-f1d2-db8ea4e300d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ìƒ˜í”Œ ë¦¬ìŠ¤íŠ¸ ìœ„ì¹˜: /content/drive/MyDrive/BOAZminiproject1/fulldata/lists/train_pairs_joint.small.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "up = Path(\"/content/MWFormer/utils_network.py\")\n",
        "src = up.read_text()\n",
        "\n",
        "# ì˜ëª» ë“¤ì–´ê°„ ì¤‘ë³µ ì œê±°\n",
        "src = src.replace(\"data_range=1, data_range=1.0\", \"data_range=1.0\")\n",
        "src = src.replace(\"data_range=1.0, data_range=1.0\", \"data_range=1.0\")\n",
        "\n",
        "# í˜¹ì‹œ ë‚¨ì•„ìˆì„ ì¤‘ë³µ ëŒ€ë¹„: \"data_range=1,\" -> \"data_range=1.0,\"\n",
        "src = src.replace(\"data_range=1,\", \"data_range=1.0,\")\n",
        "\n",
        "up.write_text(src)\n",
        "print(\"âœ… utils_network.py ì¤‘ë³µ data_range ì¸ì ì œê±° ì™„ë£Œ\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aH2OZCrMEnz",
        "outputId": "c2e0933f-bdcd-49de-fa35-9622b28c1ff0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… utils_network.py ì¤‘ë³µ data_range ì¸ì ì œê±° ì™„ë£Œ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== ë¡œì»¬ ìºì‹œ + ì§„í–‰ë¥ /ETA + ë§ˆì¼ìŠ¤í†¤(20/40/60/80/100%) ìë™ ë°±ì—… ======\n",
        "from pathlib import Path\n",
        "import shutil, os, time, math, json, subprocess, datetime\n",
        "\n",
        "# === ê²½ë¡œ ì„¤ì • ===\n",
        "DRIVE      = Path(\"/content/drive/MyDrive/BOAZminiproject1\")\n",
        "SRC_ROOT   = DRIVE / \"fulldata\"\n",
        "SRC_LIST   = SRC_ROOT / \"lists\" / \"train_pairs_joint.txt\"   # ë‘ ì¹¼ëŸ¼ (input  gt)\n",
        "\n",
        "DST_ROOT   = Path(\"/content/fastdata\")                      # ìºì‹œ íƒ€ê¹ƒ(ë¡œì»¬ SSD)\n",
        "DST_LIST   = DST_ROOT / \"lists\" / \"train_pairs_joint.txt\"   # ì„±ê³µ ìŒë§Œ ë‹¤ì‹œ ê¸°ë¡\n",
        "\n",
        "# === ë“œë¼ì´ë¸Œ ë°±ì—… ê²½ë¡œ (ë¯¸ëŸ¬) ===\n",
        "BACKUP_DIR = DRIVE / \"fastdata_cache\"                       # ë¯¸ëŸ¬ í´ë”(ë®ì–´ì“°ê¸°/ì¦ë¶„)\n",
        "BACKUP_TAG_DIR = DRIVE / \"fastdata_cache_milestones\"        # ì„ íƒ: ìŠ¤ëƒ…ìƒ·(ì˜µì…˜)\n",
        "\n",
        "# === ìºì‹œ/ë¡œê·¸ ì˜µì…˜ ===\n",
        "REPORT_EVERY      = 1000   # nìŒë§ˆë‹¤ ë¡œê·¸\n",
        "REPORT_EVERY_SEC  = 10     # ë˜ëŠ” nì´ˆë§ˆë‹¤ ë¡œê·¸\n",
        "RETRIES           = 6      # I/O ì¬ì‹œë„\n",
        "BASE_DELAY        = 0.4    # ì¬ì‹œë„ ì§€ì—°(ì§€ìˆ˜ ì¦ê°€)\n",
        "MILESTONES        = [20, 40, 60, 80, 100]   # % ë„ë‹¬ ì‹œ ë°±ì—…\n",
        "MAKE_SNAPSHOT_COPY = False  # Trueë©´ ê° ë§ˆì¼ìŠ¤í†¤ë³„ ë³„ë„ ìŠ¤ëƒ…ìƒ· í´ë”ë„ ìƒì„±(ìš©ëŸ‰ ì£¼ì˜)\n",
        "\n",
        "# ====== Helper ======\n",
        "def safe_exists(p: Path, retries=RETRIES, base_delay=BASE_DELAY):\n",
        "    p = Path(p)\n",
        "    for i in range(retries):\n",
        "        try:\n",
        "            return p.exists()\n",
        "        except OSError:\n",
        "            time.sleep(base_delay * (i + 1))\n",
        "    return False\n",
        "\n",
        "def safe_getsize(p: Path, retries=RETRIES, base_delay=BASE_DELAY):\n",
        "    p = Path(p)\n",
        "    for i in range(retries):\n",
        "        try:\n",
        "            return os.path.getsize(p)\n",
        "        except OSError:\n",
        "            time.sleep(base_delay * (i + 1))\n",
        "    return None\n",
        "\n",
        "def same_size(src: Path, dst: Path):\n",
        "    s = safe_getsize(src)\n",
        "    d = safe_getsize(dst)\n",
        "    return (s is not None) and (d is not None) and (s == d)\n",
        "\n",
        "def copy2_with_retry(src: Path, dst: Path, retries=RETRIES, base_delay=BASE_DELAY, verify=True):\n",
        "    src = Path(src); dst = Path(dst)\n",
        "    dst.parent.mkdir(parents=True, exist_ok=True)\n",
        "    last_err = None\n",
        "    for i in range(retries):\n",
        "        try:\n",
        "            shutil.copy2(src, dst)\n",
        "            if not verify or same_size(src, dst):\n",
        "                return True\n",
        "        except OSError as e:\n",
        "            last_err = e\n",
        "            time.sleep(base_delay * (i + 1))\n",
        "    # ë§ˆì§€ë§‰ ê²€ì¦\n",
        "    if verify and dst.exists() and same_size(src, dst):\n",
        "        return True\n",
        "    if last_err:\n",
        "        raise last_err\n",
        "    return False\n",
        "\n",
        "def fmt_time(sec):\n",
        "    if sec is None or math.isinf(sec) or sec < 0:\n",
        "        return \"--:--:--\"\n",
        "    h = int(sec // 3600); m = int((sec % 3600) // 60); s = int(sec % 60)\n",
        "    return f\"{h:02d}:{m:02d}:{s:02d}\"\n",
        "\n",
        "def write_manifest(path: Path, payload: dict):\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(path, \"w\") as f:\n",
        "        json.dump(payload, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "def rsync_available():\n",
        "    try:\n",
        "        out = subprocess.run([\"rsync\", \"--version\"], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "        return out.returncode == 0\n",
        "    except FileNotFoundError:\n",
        "        return False\n",
        "\n",
        "def backup_to_drive(src_dir: Path, dst_dir: Path, milestone_pct: int):\n",
        "    \"\"\"\n",
        "    /content/fastdata â†’ Driveë¡œ ì¦ë¶„ ë°±ì—…(ê°€ëŠ¥í•˜ë©´ rsync ì‚¬ìš©).\n",
        "    milestone_pct: ì§„í–‰ë¥ (ì •ìˆ˜)\n",
        "    \"\"\"\n",
        "    src_dir = Path(src_dir); dst_dir = Path(dst_dir)\n",
        "    dst_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    start = time.time()\n",
        "    print(f\"ğŸ”„ ë°±ì—… ì‹œì‘({milestone_pct}%): {src_dir} â†’ {dst_dir}\")\n",
        "\n",
        "    # 1) rsyncê°€ ìˆìœ¼ë©´ rsyncë¡œ ë¹ ë¥´ê²Œ ë™ê¸°í™”(ì¦ë¶„)\n",
        "    if rsync_available():\n",
        "        # -a (archive), -r (recursive), -t (times), --delete(ì‚­ì œ ë™ê¸°í™”), --update(ìƒˆë¡œìš´/ìˆ˜ì •ëœ íŒŒì¼ë§Œ)\n",
        "        cmd = [\n",
        "            \"rsync\", \"-a\", \"--delete\", \"--update\",\n",
        "            f\"{str(src_dir).rstrip('/')}/\", f\"{str(dst_dir).rstrip('/')}/\"\n",
        "        ]\n",
        "        proc = subprocess.run(cmd)\n",
        "        if proc.returncode != 0:\n",
        "            print(\"âš ï¸ rsync ì‹¤íŒ¨ â†’ shutilë¡œ í´ë°±í•©ë‹ˆë‹¤.\")\n",
        "            _copytree_update(src_dir, dst_dir)\n",
        "    else:\n",
        "        # 2) rsyncê°€ ì—†ìœ¼ë©´ ëŠë¦¬ì§€ë§Œ íŒŒì´ì¬ìœ¼ë¡œ ì¦ë¶„ ë³µì‚¬\n",
        "        _copytree_update(src_dir, dst_dir)\n",
        "\n",
        "    # ë§ˆì¼ìŠ¤í†¤ ë§ˆì»¤/ë©”íƒ€ ì €ì¥\n",
        "    marker = dst_dir / f\"_milestone_{milestone_pct}p.txt\"\n",
        "    marker.write_text(f\"milestone: {milestone_pct}%\\nfinished_at: {datetime.datetime.now().isoformat()}\")\n",
        "\n",
        "    meta = {\n",
        "        \"milestone_pct\": milestone_pct,\n",
        "        \"finished_at\": datetime.datetime.now().isoformat()\n",
        "    }\n",
        "    write_manifest(dst_dir / \"cache_backup_meta.json\", meta)\n",
        "\n",
        "    # (ì˜µì…˜) ìŠ¤ëƒ…ìƒ· í´ë” ë³„ë„ ìƒì„±\n",
        "    if MAKE_SNAPSHOT_COPY:\n",
        "        SNAP = Path(BACKUP_TAG_DIR) / f\"milestone_{milestone_pct:03d}p\"\n",
        "        print(f\"ğŸ“¦ ìŠ¤ëƒ…ìƒ· ì €ì¥: {SNAP}\")\n",
        "        if rsync_available():\n",
        "            SNAP.parent.mkdir(parents=True, exist_ok=True)\n",
        "            cmd2 = [\"rsync\", \"-a\", \"--delete\", \"--update\",\n",
        "                    f\"{str(dst_dir).rstrip('/')}/\", f\"{str(SNAP).rstrip('/')}/\"]\n",
        "            subprocess.run(cmd2)\n",
        "        else:\n",
        "            _copytree_update(dst_dir, SNAP)\n",
        "\n",
        "    took = fmt_time(time.time() - start)\n",
        "    print(f\"âœ… ë°±ì—… ì™„ë£Œ({milestone_pct}%) | ì†Œìš”: {took}\")\n",
        "\n",
        "def _copytree_update(src_root: Path, dst_root: Path):\n",
        "    \"\"\"dstì— ê°™ì€ ê²½ë¡œ/ì´ë¦„ì´ ìˆê³  ì‚¬ì´ì¦ˆ ê°™ìœ¼ë©´ ìŠ¤í‚µ, ì•„ë‹ˆë©´ ì—…ë°ì´íŠ¸ ë³µì‚¬\"\"\"\n",
        "    src_root = Path(src_root); dst_root = Path(dst_root)\n",
        "    for root, dirs, files in os.walk(src_root):\n",
        "        rel = os.path.relpath(root, src_root)\n",
        "        dst_dir = os.path.join(dst_root, rel) if rel != \".\" else str(dst_root)\n",
        "        os.makedirs(dst_dir, exist_ok=True)\n",
        "        for fn in files:\n",
        "            s = Path(root) / fn\n",
        "            d = Path(dst_dir) / fn\n",
        "            try:\n",
        "                if d.exists() and same_size(s, d):\n",
        "                    continue\n",
        "                shutil.copy2(s, d)\n",
        "            except Exception:\n",
        "                # íŒŒì¼ ë‹¨ìœ„ ì—ëŸ¬ëŠ” ë¬´ì‹œí•˜ê³  ê³„ì† ì§„í–‰(ëŒ€ë¶€ë¶„ ì¼ì‹œì  I/O)\n",
        "                pass\n",
        "\n",
        "# ====== ë¦¬ìŠ¤íŠ¸ ì½ê¸° ======\n",
        "assert safe_exists(SRC_LIST), f\"ë¦¬ìŠ¤íŠ¸ê°€ ë³´ì´ì§€ ì•ŠìŠµë‹ˆë‹¤: {SRC_LIST}\\në“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.\"\n",
        "with open(SRC_LIST) as f:\n",
        "    raw_lines = [ln.strip() for ln in f if ln.strip()]\n",
        "\n",
        "pairs_all = []\n",
        "for ln in raw_lines:\n",
        "    parts = ln.split()\n",
        "    if len(parts) >= 2:\n",
        "        pairs_all.append((parts[0].lstrip(\"/\"), parts[1].lstrip(\"/\")))\n",
        "TOTAL = len(pairs_all)\n",
        "print(f\"ì´ ìŒ ìˆ˜: {TOTAL:,}\")\n",
        "\n",
        "# ====== ìºì‹œ ë£¨í”„ ======\n",
        "ok_pairs, copied_files, skipped_files, missing, failed = 0, 0, 0, 0, 0\n",
        "pairs_ok_list = []\n",
        "\n",
        "start = time.time()\n",
        "last_report_t = start\n",
        "# ë§ˆì¼ìŠ¤í†¤ ê´€ë¦¬(í•œ ë²ˆë§Œ ìˆ˜í–‰)\n",
        "pending_milestones = set(MILESTONES)\n",
        "\n",
        "# ë¯¸ë¦¬ í´ë” ìƒì„±\n",
        "(DST_ROOT / \"lists\").mkdir(parents=True, exist_ok=True)\n",
        "BACKUP_DIR.mkdir(parents=True, exist_ok=True)\n",
        "BACKUP_TAG_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "for idx, (inp_rel, gt_rel) in enumerate(pairs_all, 1):\n",
        "    s_inp, s_gt = SRC_ROOT / inp_rel, SRC_ROOT / gt_rel\n",
        "    d_inp, d_gt = DST_ROOT / inp_rel, DST_ROOT / gt_rel\n",
        "    d_inp.parent.mkdir(parents=True, exist_ok=True)\n",
        "    d_gt.parent.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    ok = True\n",
        "\n",
        "    # input\n",
        "    if safe_exists(s_inp):\n",
        "        if safe_exists(d_inp) and same_size(s_inp, d_inp):\n",
        "            skipped_files += 1\n",
        "        else:\n",
        "            try:\n",
        "                copy2_with_retry(s_inp, d_inp)\n",
        "                copied_files += 1\n",
        "            except OSError:\n",
        "                ok = False\n",
        "                failed += 1\n",
        "    else:\n",
        "        ok = False\n",
        "        missing += 1\n",
        "\n",
        "    # gt\n",
        "    if safe_exists(s_gt):\n",
        "        if safe_exists(d_gt) and same_size(s_gt, d_gt):\n",
        "            skipped_files += 1\n",
        "        else:\n",
        "            try:\n",
        "                copy2_with_retry(s_gt, d_gt)\n",
        "                copied_files += 1\n",
        "            except OSError:\n",
        "                ok = False\n",
        "                failed += 1\n",
        "    else:\n",
        "        ok = False\n",
        "        missing += 1\n",
        "\n",
        "    if ok:\n",
        "        ok_pairs += 1\n",
        "        pairs_ok_list.append(f\"{inp_rel} {gt_rel}\")\n",
        "\n",
        "    # ì§„í–‰ë¥ /ETA ì¶œë ¥\n",
        "    now = time.time()\n",
        "    need_time_report = (idx % REPORT_EVERY == 0) or (now - last_report_t >= REPORT_EVERY_SEC)\n",
        "    if need_time_report:\n",
        "        elapsed = now - start\n",
        "        pct_idx = idx / TOTAL * 100 if TOTAL else 0.0\n",
        "        pct_ok  = ok_pairs / TOTAL * 100 if TOTAL else 0.0\n",
        "        sec_per_pair = elapsed / idx if idx else float(\"inf\")\n",
        "        eta_sec = sec_per_pair * (TOTAL - idx)\n",
        "        print(\n",
        "            f\"[{idx:,}/{TOTAL:,}] \"\n",
        "            f\"done={ok_pairs:,} ({pct_ok:5.1f}%)  \"\n",
        "            f\"progress={pct_idx:5.1f}%  \"\n",
        "            f\"copied={copied_files:,}  skipped={skipped_files:,}  \"\n",
        "            f\"missing={missing:,}  failed={failed:,}  \"\n",
        "            f\"elapsed={fmt_time(elapsed)}  ETA={fmt_time(eta_sec)}\"\n",
        "        )\n",
        "        last_report_t = now\n",
        "\n",
        "    # === ë§ˆì¼ìŠ¤í†¤ ë„ë‹¬ ì‹œ ìë™ ë°±ì—… ===\n",
        "    done_pct_int = int(ok_pairs / TOTAL * 100) if TOTAL else 0\n",
        "    hit = sorted([m for m in pending_milestones if done_pct_int >= m])\n",
        "    if hit:\n",
        "        # ì¤‘ë³µ ë°©ì§€: ê°€ì¥ ë‚®ì€ ë§ˆì¼ìŠ¤í†¤ë¶€í„° ìˆœì„œëŒ€ë¡œ ì‹¤í–‰ í›„ pendingì—ì„œ ì œê±°\n",
        "        for m in hit:\n",
        "            try:\n",
        "                backup_to_drive(DST_ROOT, BACKUP_DIR, m)\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ ë§ˆì¼ìŠ¤í†¤ {m}% ë°±ì—… ì‹¤íŒ¨: {e}\")\n",
        "            pending_milestones.discard(m)\n",
        "\n",
        "# ë¦¬ìŠ¤íŠ¸ ì €ì¥(ì„±ê³µ ìŒë§Œ)\n",
        "DST_LIST.parent.mkdir(parents=True, exist_ok=True)\n",
        "with open(DST_LIST, \"w\") as w:\n",
        "    w.write(\"\\n\".join(pairs_ok_list))\n",
        "\n",
        "elapsed = time.time() - start\n",
        "print(\"\\nâœ… ë¡œì»¬ ìºì‹œ ì™„ë£Œ\")\n",
        "print(f\"ìŒ(OK)   : {ok_pairs:,} / {TOTAL:,}\")\n",
        "print(f\"ë³µì‚¬(íŒŒì¼): {copied_files:,}  | ìŠ¤í‚µ: {skipped_files:,}  | ëˆ„ë½: {missing:,}  | ì‹¤íŒ¨: {failed:,}\")\n",
        "print(f\"ê²½ê³¼ì‹œê°„  : {fmt_time(elapsed)}\")\n",
        "print(\"ë¡œì»¬ ë¦¬ìŠ¤íŠ¸:\", DST_LIST)\n",
        "\n",
        "# ìµœì¢… ë§ˆë‹ˆí˜ìŠ¤íŠ¸ + ë§ˆì§€ë§‰ ë°±ì—…(í˜¹ì‹œ 100% ë§ˆì¼ìŠ¤í†¤ì„ ëª» íƒ”ë‹¤ë©´ í•œ ë²ˆ ë”)\n",
        "manifest = {\n",
        "    \"ok_pairs\": ok_pairs,\n",
        "    \"total_pairs\": TOTAL,\n",
        "    \"elapsed\": elapsed,\n",
        "    \"finished_at\": datetime.datetime.now().isoformat(),\n",
        "}\n",
        "write_manifest(DST_ROOT / \"cache_progress.json\", manifest)\n",
        "\n",
        "if 100 in pending_milestones:\n",
        "    try:\n",
        "        backup_to_drive(DST_ROOT, BACKUP_DIR, 100)\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ ìµœì¢… ë°±ì—… ì‹¤íŒ¨: {e}\")\n",
        "\n",
        "print(\"ğŸ‰ ìºì‹± & ë§ˆì¼ìŠ¤í†¤ ë°±ì—… ì¢…ë£Œ\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 242
        },
        "id": "GKZya7tjOSel",
        "outputId": "ec2d57f0-b604-4c5b-b1c0-cac7ef444a72"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ë¦¬ìŠ¤íŠ¸ê°€ ë³´ì´ì§€ ì•ŠìŠµë‹ˆë‹¤: /content/drive/MyDrive/BOAZminiproject1/fulldata/lists/train_pairs_joint.txt\në“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1585099026.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;31m# ====== ë¦¬ìŠ¤íŠ¸ ì½ê¸° ======\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0msafe_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSRC_LIST\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"ë¦¬ìŠ¤íŠ¸ê°€ ë³´ì´ì§€ ì•ŠìŠµë‹ˆë‹¤: {SRC_LIST}\\në“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸ë¥¼ í™•ì¸í•˜ì„¸ìš”.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSRC_LIST\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0mraw_lines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mln\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mln\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mln\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: ë¦¬ìŠ¤íŠ¸ê°€ ë³´ì´ì§€ ì•ŠìŠµë‹ˆë‹¤: /content/drive/MyDrive/BOAZminiproject1/fulldata/lists/train_pairs_joint.txt\në“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸ë¥¼ í™•ì¸í•˜ì„¸ìš”."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##ìºì‹± ëë‚˜ë©´ ë¬´ì¡°ê±´ ë“œë¼ì´ë¸Œì— ë„£ì„ ê²ƒ!!!!"
      ],
      "metadata": {
        "id": "Y_uKQWZ-BneG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ìˆ˜ë™ ë“œë¼ì´ë¸Œ ì €ì¥\n",
        "# !cp -r /content/fastdata /content/drive/MyDrive/BOAZminiproject1/fastdata_cache"
      ],
      "metadata": {
        "id": "ToArWns-Bmfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ë‚˜ì¤‘ì— ë¶ˆëŸ¬ì˜¤ê¸°\n",
        "!cp -r /content/drive/MyDrive/BOAZminiproject1/fastdata_cache /content/fastdata"
      ],
      "metadata": {
        "id": "rGQjyl2yEwrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ìµœì†Œ íŒ¨ì¹˜(AMP + ì‘ì€ ë°°ì¹˜ + ë“œë¼ì´ë¸Œ I/O ì¬ì‹œë„)"
      ],
      "metadata": {
        "id": "nF0DjW3QKs96"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === ìµœì†Œ íŒ¨ì¹˜: main_finetune(AMP/ì‘ì€ë°°ì¹˜) + train_data_functions(open_with_retry) ===\n",
        "from pathlib import Path, re as _re\n",
        "\n",
        "# ---------- main_finetune.py ----------\n",
        "mp = Path(\"/content/MWFormer/main_finetune.py\")\n",
        "msrc = mp.read_text()\n",
        "\n",
        "# (A) AMP import (ì¤‘ë³µ ë°©ì§€)\n",
        "if \"from torch.cuda.amp import autocast, GradScaler\" not in msrc:\n",
        "    msrc = msrc.replace(\n",
        "        \"from datetime import datetime\",\n",
        "        \"from datetime import datetime\\nfrom torch.cuda.amp import autocast, GradScaler\"\n",
        "    )\n",
        "\n",
        "# (B) scaler ì´ˆê¸°í™” (ì¤‘ë³µ ë°©ì§€)\n",
        "if \"scaler = GradScaler()\" not in msrc:\n",
        "    msrc = msrc.replace(\n",
        "        \"model.train()\",\n",
        "        \"model.train()\\n\\n    # AMP scaler (ë©”ëª¨ë¦¬ ì ˆì•½)\\n    scaler = GradScaler()\"\n",
        "    )\n",
        "\n",
        "# (C) í•™ìŠµ ë£¨í”„ì˜ backward/stepì„ AMPë¡œ ê°ì‹¸ê¸° (íŒ¨í„´ì´ ë‹¤ë¥´ë©´ ê±´ë„ˆëœ€)\n",
        "if \"scaler.scale(\" not in msrc:\n",
        "    msrc = _re.sub(\n",
        "        r\"opts\\.zero_grad\\(\\)\\s*\\n([\\s\\S]*?)loss_p\\s*=\\s*loss_calc\\((.*?)\\)\\s*\\n\\s*loss_p\\.backward\\(\\)\\s*\\n\\s*opts\\.step\\(\\)\",\n",
        "        (\n",
        "            \"opts.zero_grad()\\n\"\n",
        "            \"\\\\1with autocast():\\n\"\n",
        "            \"            loss_p = loss_calc(\\\\2)\\n\"\n",
        "            \"        scaler.scale(loss_p).backward()\\n\"\n",
        "            \"        scaler.step(opts)\\n\"\n",
        "            \"        scaler.update()\"\n",
        "        ),\n",
        "        msrc,\n",
        "        count=1\n",
        "    )\n",
        "\n",
        "# (D) DataLoader ë°°ì¹˜ í¬ê¸° ìµœì†Œí™”(10/11/4 â†’ 2), ì—¬ëŸ¬ ê³³ì— ìˆì„ ìˆ˜ ìˆì–´ ì¹˜í™˜\n",
        "msrc = msrc.replace(\"batch_size=10\", \"batch_size=2\")\n",
        "msrc = msrc.replace(\"batch_size=11\", \"batch_size=2\")\n",
        "msrc = msrc.replace(\"batch_size=4\",  \"batch_size=2\")\n",
        "\n",
        "mp.write_text(msrc)\n",
        "print(\"âœ… main_finetune.py: AMP/ì‘ì€ë°°ì¹˜ íŒ¨ì¹˜ ì™„ë£Œ\")\n",
        "\n",
        "# ---------- train_data_functions.py ----------\n",
        "tp = Path(\"/content/MWFormer/train_data_functions.py\")\n",
        "tsrc = tp.read_text()\n",
        "\n",
        "# (E) open_with_retry í—¬í¼ ì‚½ì…(ì¤‘ë³µ ë°©ì§€)\n",
        "if \"def open_with_retry(\" not in tsrc:\n",
        "    helper = r\"\"\"\n",
        "\n",
        "import time\n",
        "from PIL import Image\n",
        "\n",
        "def open_with_retry(img_path, max_retries=6, delay=0.4):\n",
        "    last_err = None\n",
        "    for _ in range(max_retries):\n",
        "        try:\n",
        "            with Image.open(img_path) as im:\n",
        "                return im.convert('RGB')\n",
        "        except Exception as e:\n",
        "            last_err = e\n",
        "            time.sleep(delay)\n",
        "    raise last_err\n",
        "\"\"\"\n",
        "    tsrc = tsrc.replace(\"ImageFile.LOAD_TRUNCATED_IMAGES = True\", \"ImageFile.LOAD_TRUNCATED_IMAGES = True\\n\" + helper)\n",
        "\n",
        "# (F) ì‹¤ì œ ì´ë¯¸ì§€ ë¡œë”© ë¶€ë¶„ì„ open_with_retryë¡œ êµì²´(ì¡´ì¬ ì‹œì—ë§Œ)\n",
        "tsrc = _re.sub(\n",
        "    r\"input_img = Image\\.open\\(self\\.train_data_dir \\+ input_name\\)\\s*?\\n\\s*try:\\s*?\\n\\s*gt_img = Image\\.open\\(self\\.train_data_dir \\+ gt_name\\)\\s*?\\n\\s*except:\\s*?\\n\\s*gt_img = Image\\.open\\(self\\.train_data_dir \\+ gt_name\\)\\.convert\\('RGB'\\)\",\n",
        "    \"input_img = open_with_retry(self.train_data_dir + input_name)\\n        gt_img    = open_with_retry(self.train_data_dir + gt_name)\",\n",
        "    tsrc\n",
        ")\n",
        "\n",
        "# (G) ê²½ê³  ìˆ˜ì •: 'is not 3' â†’ '!= 3'\n",
        "tsrc = tsrc.replace(\" is not 3\", \" != 3\")\n",
        "\n",
        "tp.write_text(tsrc)\n",
        "print(\"âœ… train_data_functions.py: open_with_retry/ê²½ê³ ìˆ˜ì • íŒ¨ì¹˜ ì™„ë£Œ\")"
      ],
      "metadata": {
        "id": "ex9PTsE6K7VJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ê²½ë¡œ/ë¦¬ìŠ¤íŠ¸ ì ê²€(ë¹ ë¥´ê²Œ)"
      ],
      "metadata": {
        "id": "5TO_Dz0JK8Zt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "FAST_ROOT = Path(\"/content/fastdata\")\n",
        "LIST_PATH = FAST_ROOT / \"lists\" / \"train_pairs_joint.txt\"\n",
        "\n",
        "print(\"fastdata ì¡´ì¬:\", FAST_ROOT.exists())\n",
        "print(\"train_pairs_joint.txt ì¡´ì¬:\", LIST_PATH.exists())\n",
        "if LIST_PATH.exists():\n",
        "    print(\"ë¼ì¸ ìˆ˜:\", sum(1 for _ in open(LIST_PATH)))"
      ],
      "metadata": {
        "id": "hSPAiu31K-n9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "í•™ìŠµ ì‹¤í–‰(ìºì‹œ ì‚¬ìš©, ì•ˆì • ì„¸íŒ…)"
      ],
      "metadata": {
        "id": "s3VPCRwrLAJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/MWFormer\n",
        "\n",
        "!python main_finetune.py \\\n",
        "  -train_data_dir /content/fastdata/ \\\n",
        "  -labeled_name lists/train_pairs_joint.txt \\\n",
        "  -restore-from-stylefilter /content/drive/MyDrive/BOAZminiproject1/MWFormer_weights/pretrained_feature_extraction_network/style_filter.pth \\\n",
        "  -restore-from /content/drive/MyDrive/BOAZminiproject1/MWFormer_weights/MWFormer_L/backbone.pth \\\n",
        "  -num-workers 2 \\\n",
        "  -crop_size 256 256 \\\n",
        "  -save-pred-every 10000 \\\n",
        "  -loss_save_step 10000 \\\n",
        "  -snapshot-dir /content/drive/MyDrive/BOAZminiproject1/finetunning/snapshots \\\n",
        "  -file-name MWFormer_L_wandb_fast"
      ],
      "metadata": {
        "id": "0CS2WiTLLBR_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}